%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:intro}

The interaction between conductors and musicians
is a sophisticated form of non-verbal communication.
While the expressions used in conducting have no strict rules,
most gestures performed by an experienced conductor are understood by
adequately experienced musicians.
In order to model this complex conductor-musician interaction,
it is vital to understand the fundamental principles behind the communication.
It is then possible to capture relevant data
and combine it with a musical score to
produce an interactive sonic experience.

\section{Conducting Gestures}

\fixme{Summarize what a modern conductor does}

While some forms of musical conducting have been around for hundreds of years
[Green 1961, look up from gallops 2005],
the developments that lead to the modern form of conducting
were driven by the increasing size and complexity of symphonic scores
in the late nineteenth century. \cite{gallops2005}
Studying conducting from a scientific, data based approach,
is an even more recent development.
Sousa \cite{sousa1988} was among the first,
performing a study in 1988,
which investigated the use of musical conducting emblems,
and their interpretation by instrumental performers.
Sousa used videotaped conducting gestures,
which he presented to university, high school and junior high school students,
to study which gestures could be classified as conducting emblems,
non-verbal acts with precise meaning and
a common interpretation among instrumental performers.
He concluded that 38 out of the 55 studied gestures
were recognised by over 70\% of the subjects,
with the recognition rate having a strong correlation
with the experience level of the musicians.

While some conducting gestures can be easily analysed,
and do have a commonly recognised meaning,
to really understand how the communication between
conductors and musicians works,
one has to study the whole life cycle of a conducted performance -
preparations made by the conductor before rehearsing with the musicians,
rehearsal with musicians,
and finally the actual performance.
Konttinen \cite{konttinen2008} studied
conducting as a practical and sociological activity
in her dissertation \textit{Conducting Gestures}.
She came to the conclusion that the main purpose of all
conducting gestures become apparent in a social situation -
a rehearsal, performance or conducting class -
where the communicational situation includes both the gesture
and the social context in which it is used.
Therefore the meaning of conducting gestures are
heavily influenced by their social context -
the musicians affecting the way the conductor performs,
and the conductor affecting the way the musicians interpret the gestures.
Especially the rehearsal situation,
where the conductor needs to make the meaning of his gestures obvious,
is crucial for founding the basis for successful gestural communication.

\section{Gesture-Based Human-Computer Interaction}

When using gestures for human-computer interaction,
the device used for gathering data plays a big role
in the capabilities and limitations of the system.
Early gesture systems were based on input from
a camera or a specially made input device [citation needed].
Recent developments in technology,
such as popularization of touchscreen smartphones,
have made gestures as a form of human-computer interaction
a part of everyday life for an increasing amount of people.

Depth sensor equipped motion sensing input devices,
such as the Mictosoft Kinect \cite{kinect_overview},
represent a recent trend of incorporating gesture based interaction
into consumer electronics.
Interaction with applications using these devices
does not require touching or wearing any equipment,
and thus the applications have the potential to provide
a very natural gestural user interface.
However, the complexity of the matter lies in extracting
meaningful information out of the raw motion data provided by these devices.
\fixme{quote some studies and methods used}


\section{Expressive Sound Synthesis}

Music is fundamentally a form of human expression [quotation needed],
and thus expressiveness a fundamental property of music.
Therefore one could argue that the ultimate goal of sound synthesis should be to support
the expressive motives of the composer and performer of the music.
However, most control parameters available in sound synthesis systems
are not related to the expressive properties of the produced sound,
but are rather based on the time-frequency properties of the signal.
Various studies have been made on the relation between
descriptive and emotional adjectives,
and the time-frequency properties of sound.
The studies have covered both musical cues,
such as tempo, loudness and articulation \cite{juslin2000cue},
and the timbre of individual notes \cite{moravec2005}.

The majority of motion based expressive sound synthesis implementations
have been novel approaches which do not model any existing form of musical expression.
Some have concentrated on using emotional data
extracted from the motion of the user,
to control music in a way that the
emotional features of the music would match those of the user \cite{friberg2004},
while others have provided means create mappings
between arbitrary motion data and sound features \cite{kia2004}.
\fixme{Mention virtual instruments}

Because bringing out expressive features of music is a crucial part of conducting,
a conductor follower is a rather natural candidate for
an expressive sound synthesis system.

\section{Conductor Follower Systems}

Over the years, several performance, educational and research systems
have been designed and implemented,
which follow conductor gestures and produce or modify music based on the input.
\fixme{History covered in detail in several other papers, just a quick overview here}

\fixme{Summarize what we are doing and why}

\section{Thesis Overview}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods}
\label{chapter:methods}

\fixme{Write some more}

\section{Motion Capture}

Motion capture in the context of the conductor follower system,
consists of tracking the hands.
\fixme{Will this still change?}
The methods used for this are fairly mature,
and not in the main focus of this thesis.
Since sufficient methods and implementations for
motion capture using depth sensor devices
are commonly available,
there is no need to cover the details of those here.

\fixme{Write more, once the implementation is more final}

\subsection{Feature Extraction}

Luck and Toiviainen have studied the gestures of conductors
both from a beat synchronization \cite{luck2006}
and expressive \cite{luck2010} point of view.
Both studies used Savitzky-Golay filtering \cite{savitzky1964}
for data smoothing and differentiation.
As the results of these studies are utilized,
it is logical to use the same methods in the implementation.

\subsection{Beat Detection}
\label{sec:meth:beat_detection}

\fixme{Tell that we want a general method, not specific patterns}

Luck and Toiviainen \cite{luck2006}
conducted a beat synchronization study,
which was novel in the sense that it studied
the subject in a real orchestral rehearsal situation.
Previous studies have been made primarily in laboratory settings,
and thus do not present a good foundation
for emulating the behavior of an orchestra.
The study analyzed the motion of the baton
in four excerpts of conducting,
collected from one rehearsal situation.
Since the study represented the conducting style of only one conductor,
and the response of only one orchestra,
it does not present universally valid results.
However, looking at both laboratory and rehearsal situation based studies showed
that two features stood out as good candidates for beat detection:
vertical position and velocity.
Both have been shown to have a
rather strong correlation with beat induction \cite{add citations}.
Luck and Toiviainen's study showed that when the conductor's gestures were well articulated,
the lowest vertical position preceded the beat by circa 230 milliseconds,
while the highest vertical velocity preceded the beat by circa 90 milliseconds,
both having a high correlation at that lag.

\section{Score Following}

\fixme{Write something}

\subsection{Score Data Format}

Regardless of its limitations and age,
MIDI (Musical Instrument Digital Interface)
is still one of the most used standards
for the digital representation of of musical events.
MIDI is also the built-in way to communicate musical events
in the Virtual Studio Technology (VST) plugin format,
which is used by VSL.
Thus using MIDI as the score format was a natural choice.

A MIDI score does contain
note, time signature and tempo information.
It may also contain \textit{program change} events,
as specified in the General Midi specification \cite{},
to indicate the instrument to be used for each track.
However, practical experience with MIDI files showed,
that using program changes to deduce the instrument to use,
was not sufficient.
Often scores would use the
\textit{String ensemble} patch for all strings,
even though the string instruments were separated to individual tracks.
In cases like this it is necessary to be able to manually
define the instrument to be used with each track.

In addition to track instrument assignment,
the system should also be aware of events in the score.
\fixme{Write about score events}

\subsection{Real Time and Score Positions}

When modifying the playback speed of a musical score
which has tempo variations already build into it,
it is important to precisely define the concepts of time used.
Wall-clock time is the monotonically progressing concept of time
most familiar to humans.
It is the time counted by a regular clock,
and will be referred to as real time,
its dimension denoted by $T$ and quantities with $t$.

Score positions, on the other hand,
are slightly more complex.
Musical events in a fully defined score have two time bases:
\begin{enumerate}
\item Musical time has the base unit of beats, but also includes a time varying derived unit of bars. Any position in the score can thus be expressed as either a beat offset, or a combination of bars and beats. Its dimension is denoted with $B$ and quantities with $b$.
\item Score time has the base unit of seconds. This is an offset from the beginning of the score if the original tempo is followed. Its dimension is denoted with $\Phi$ and quantities with $\varphi$.
\end{enumerate}
A score position is thus defined as
$ P := \tuple{b, \varphi}, $
and variables will be denoted with a $p$.

Mapping between real time and score positions will be denoted with
the mapping operator
$ \mathcal{M} : T \rightarrow P, $
and it's inverse
$ \mathcal{M}^{-1} : P \rightarrow T. $

The mapping operator (as well as its inverse)
requires knowledge of
the realized playback speed,
and can thus be applied reliably to
past events.
It is also possible to use the mapping operator for
estimating future events.
These estimates, however,
may produce results that differ from
from future mapping operations,
and may only be used in special cases.

The relation between real time and score time
is embedded in the mapping operator.
To formalize the relation, we can
represent the relation with a warp factor
\begin{equation}
w = \frac{\Delta \phi}{\Delta t},
\end{equation}
which is piecewise constant over time.
 

\subsection{Tempo Following}

The aim of the tempo following system
is to behave like a real orchestra would,
when reacting to a conductor.
\fixme{quote some studies on this matter, if you find some}
This implies that
\begin{enumerate}
\item The orchestra should not be confused by changes in the conducting pattern. \label{follow_point:pattern}
\item In the absence of beats, the orchestra will continue with it's previous tempo.
\item Extra beats in "unexpected" positions will not be acted upon. \label{follow_point:unexpected}
\item Sudden changes in tempo will not be immediately followed.
\end{enumerate}
Points \ref{follow_point:pattern} and \ref{follow_point:unexpected} require
that the tempo follower must have knowledge of all the possible
conducting patterns for each time signature,
and that it must be able to relate each beat to some beat in the pattern.
\fixme{Write more}

The formal objective of tempo following
is to minimize the offset between
beats conducted and
beats played by the virtual orchestra.
Once the beats are detected as described
in section \ref{sec:meth:beat_detection},
the beats have to be classified.
A beat classification is defined as a tuple
\begin{equation}
C := \tuple{\Delta b, q, \tau} \mid 
\tau \in \lbrace \mathrm{NotClassified}, \mathrm{CurrentBar}, \mathrm{NextBar} \rbrace,
\end{equation}
where $\Delta b$ is the offset,
$q$ the classification quality,
and $\tau$ the type.
Each beat pattern has a related
classification operation $ \mathcal{C} : B \rightarrow C, $
which uses the relative position of the detected beat
within the current bar,
to produce a classification.
Out of the set of classifications produced by the beat patterns,
the classification with the best quality is
selected as the final classification.

The action taken based on the winning classification
depends on the type of the classification.
In the case of NotClassified, no action is taken for that beat.
When the type is NextBar,
the current bar is incremented,
and the classification is repeated with an
updated relative beat position.
The type CurrentBar,
means that the offset $\Delta b$ is used for correcting
the current warp factor $w$.
The 


\fixme{offset correction}


\section{Sound Synthesis}

Since the objective of the system is to emulate an orchestra,
also the sound synthesis system should be able to
provide realistic synthesis of an entire orchestra.
It should be able to reproduce a musical composition
at varying tempi and articulations.

\subsection{Expressive Synthesis}
\label{subsec:expressive_synth}

There are several commercial orchestral synthesizers available,
the majority of which use some form of sampling or concatenative synthesis.
Based on the feature set provided and
subjective evaluation of sound quality,
Vienna Symphonic Library (VSL) \cite{vsl} was chosen for the task.
VSL uses a form of concatenative synthesis,
using a vast library of samples played at
various velocities and articulations \cite{schwartz2006}.

Each instrument in VSL contains a set of \textit{patches},
which represent a certain articulation and/or playing style.
Each patch is capable of synthesizing
the whole scale of the given instrument at several velocities.
Some patches (such as \textit{staccato} articulations)
have a limited note length,
while others allow stretching out the note infinitely.
Switching between patches can be accomplished with \textit{keyswitch} events
(notes outside of the scale of the instrument).

In order to control the switching of patches,
the synthesis system must have some knowledge of
the available instruments and their patches.
The parameters used for describing the patches are:
\begin{description}
\item[Length] The maximum note length allowed by the patch.
\item[Attack] The sharpness of the attack portion of the notes.
\item[Weight] The \textit{musical weight} of the note, i.e. an accented note would have a large weight.
\end{description}
By using this information,
together with the current motion capture and score state,
the system is capable of selecting
the most appropriate patch for each note.

\fixme{Write more on patch selection (e.g. distance function)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\label{chapter:implementation}

Since the synthesis environment is a VST plugin,
implementing the conductor follower as a VST plugin was a logical choice.
The input for the plugin is read from a MIDI file
and the output is MIDI events via the VST interface.
These implementation details are, however, hidden behind
carefully designed interfaces,
separating the core functionality from the input/output functionality.

One of the most important considerations of the implementation
is its soft real time (RT) nature.
In addition to having RT constraints,
the 30Hz frame rate of the motion capture system
makes it a multirate, multithreaded application.
Working in such an environment requires using
lock-free programming techniques,
such as atomic variables, read-copy-update (RCU) constructs,
lock-free ringbuffers and a "butler" thread.
The multirate nature also requires having robust timestamping and synchronization mechanisms.

\fixme{chapter overview}

\section{Supporting Libraries}

The implementation makes heavy use of several of the Boost C++ libraries \cite{boost}.
The most noteworthy ones are described in the following list.
Libraries included in the C++11 standard \cite{cpp11}
are denoted with $^*$,
while libraries not yet in the official boost distribution
are marked with $^\dagger$.
\begin{description}[leftmargin=14ex]
\item[Chrono$^*$] Time library for timestamping and jitter correction.
\item[Geometry] \fixme{Is this really used much anywhere?}
\item[Lockfree$^\dagger$] Various lock-free constructs.
\item[Spirit] A parsing library, used for configuration file parsing.
\item[Thread$^*$] Threading utilities.
\item[uBLAS] Linear algebra, used for polynomial fitting.
\item[Units] Compile-time dimensional analysis.
\end{description}

The OpenNI Framework \cite{openni} is an open source cross-platform
framework for Natural Interaction (NI) devices.
PrimeSense Ltd \cite{primesense} provides a proprietary
middleware package called NITE,
which works with the OpenNI API,
providing higher level functionality such as skeletal and hand tracking.
NITE is used via the OpenNI APIs for hand tracking.

Juce \cite{juce} is an open source,
multimedia oriented cross-platform C++ library.
It was used for its
VST plugin wrapper, MIDI file I/O,
and user interface (UI) functionality.

\section{Architectural overview}

The implementation is divided into
six well defined and loosely coupled modules:
\begin{itemize}
\item Common utilities
\item Data file parsers
\item Motion capture
\item Expression to synthesis parameter mapper
\item Score Follower (the core functionality)
\item The VST plugin
\end{itemize}

The common utilities module contains components
that are not specific to conductor following.
These include utilities for
lock-free programming,
mathematical operations and algorithms,
time and geometry handling,
logging, and debugging.

The data file parser module
contains all the parser definitions
and supporting data structures
for all the input files used by the system.
Its main purpose is to hide the implementation
of parsing behind a concise API.

The motion capture module provides a simple
event-based API for motion related data.
It includes the logic for extracting relevant events from motion data.
It also contains all the code that uses the OpenNI APIs,
so that the rest of the system has no dependencies on OpenNI.

The expression to synthesis parameter mapper
maps expression parameters to synthesis parameters.
\fixme{Write more}

The score follower module provides most of the core functionality,
not including motion capture.
This includes
beat classification, tempo and score following, jitter correction,
and instrument patch switching among other things.
It also provides abstract interfaces for 
hiding the implementation details of the score format.

The VST plugin modules main purpose is to
separate as much plugin implementation detail from the rest of the system.
It implements the VST interface and provides the plugin UI,
both using Juce.
It also implements the score related interfaces,
defined in the score follower module,
using Juce MIDI functionality.

In addition to these six modules,
the project contains four unit test modules.
These modules are not significant regarding the functionality,
but merely had a supporting role during system development.

\section{Time Handling}

Handling time in a multirate, multi-threaded system,
is not a trivial task.
The motion capture thread will be in
a blocking state most of the time,
waking up at a 30Hz frequency to process data
provided by the motion capture system.
The plugin, however, needs to provide MIDI data
based on the host's audio settings,
a typical configuration running the
plugin at around 100 Hz (an audio block length of 10ms).
Additionally, the thread in which the plugin runs
is controlled by the host,
and the code has to be absolutely lock-free.

Considering the implications of needing to run lock-free code on
a modern multi-core system running a general purpose
operating system (OS),
no assumptions about causality or parallelism between
motion capture and plugin code execution can be made --
scheduling latencies and parallel execution forces the use
of a reliable timestamping mechanism for synchronization between
the audio and motion capture threads.
The Boost Chrono library provides a steady clock,
which measures time since the latest system boot-up.
Acquiring the current timestamp was verified to be lock-free
at least on Windows and OS X as of Boost version 1.49.0.
These timestamps are used for measuring the interval between
events happening in the motion capture and plugin threads.
Timestamping events in the motion capture thread is straightforward,
each event simply being timestamped with
the current timestamp returned by the system.
The plugin thread will, however,
require jitter correction to prevent timing problems.

\subsection{Jitter Correction}

The plugin thread needs to know not only
the timestamp corresponding to the beginning of the current audio block,
but also an estimate for the end of the block.
Let the audio block be defined by its beginning and end:
\begin{equation}
\boldsymbol{\tau} = [\tau_b, \tau_e[.
\end{equation}
Using the information provided to the plugin,
the theoretical block length
\begin{equation}
\label{eq:theoretical_block_len}
\Delta \tau' = \frac{n_b}{f_s},
\end{equation}
where $n_b$ is the block size in samples and $f_s$ the samplerate.
This length is then used for estimating the end of each block
\begin{equation}
\tau_e = t_c + \Delta \tau',
\end{equation}
where $t_c$ is the current timestamp at the beginning of the block.
Due to scheduling latencies,
the value of $t_c$ might differ from the
previous block end estimate $\tau_{e_{\mathrm{prev}}}$.
In order to prevent gaps and overlaps between the
time estimates of consecutive audio blocks,
the beginning of each block is taken from the end of the previous block
\begin{equation}
\tau_b = \tau_{e_{\mathrm{prev}}}.
\end{equation}
The VST specification requires
each MIDI event to have its position defined
as a sample offset from the beginning of the current block.
As the actual block length
\begin{equation}
\Delta \tau = \tau_e - \tau_b
\end{equation}
will differ from the theoretical block length
$\Delta \tau'$ most of the time,
this difference will need to be taken into consideration
when calculating the event sample offset
\begin{equation}
\Delta n_e = \frac{\Delta \tau'}{\Delta \tau} (t_e - \tau_b) f_s,
\end{equation}
where $t_e$ is the event time. By using equation \ref{eq:theoretical_block_len},
this can be simplified to
\begin{equation}
\Delta n_e = n_b \frac{t_e - \tau_b}{\Delta \tau}.
\end{equation}

\section{Feature Extraction}



\subsection{Beat Detection}

\subsection{Expressive Features}

\section{Tempo Following}

\subsection{Beat Classification}

\subsection{Time Warping}

\section{Mapping Expressions to Synthesis Parameters}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{chapter:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}



