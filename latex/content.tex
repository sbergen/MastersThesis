%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:intro}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chapter:background} 

The interaction between conductors and musicians
is a sophisticated form of non-verbal communication.
While the expressions used in conducting have no strict rules,
most gestures performed by an experienced conductor are understood by
adequately experienced musicians.
In order to model this complex conductor-musician interaction,
it is vital to understand the fundamental principles behind the communication.
It is then possible to capture relevant data
and combine it with a musical score to
produce an interactive sonic experience.

\section{Conducting Gestures}

While some forms of musical conducting have been around for hundreds of years
[Green 1961, look up from heller 2005],
the developments that lead to the modern form of conducting
were driven by the increasing size and complexity of symphonic scores
in the late nineteenth century. [heller 2005, where's the original?]
Studying conducting from a scientific, data based approach,
is an even more recent development.
Sousa \cite{} was among the first,
performing a study in 1988,
which investigated the use of musical conducting emblems,
and their interpretation by instrumental performers.
Sousa used videotaped conducting gestures,
which he presented to university, high school and junior high school students,
to study which gestures could be classified as conducting emblems,
non-verbal acts with precise meaning and
a common interpretation among instrumental performers.
He concluded that 38 out of the 55 studied gestures
were recognised by over 70\% of the subjects,
with the recognition rate having a strong correlation
with the experience level of the musicians.

\fixme{More recent studies heller, konttinen, rehearsing, etc...}


\section{Gesture-Based Human-Computer Interaction}

When using gestures for human-computer interaction,
the device used for gathering data plays a big role
in the capabilities and limitations of the system.
Early gesture systems were based on input from
a camera or a specially made input device [citation needed].
Recent developments in technology,
such as popularization of touchscreen smartphones,
have made gestures as a form of human-computer interaction
a part of everyday life for an increasing amount of people.

Depth sensor equipped motion sensing input devices,
such as the Mictosoft Kinect \cite{},
represent a recent trend of incorporating gesture based interaction
into consumer electronics.
Interaction with applications using these devices
does not require touching or wearing any equipment,
and thus the applications have the potential to provide
a very natural gestural user interface.
However, the complexity of the matter lies in extracting
meaningful information out of the raw motion data provided by these devices.
\fixme{quote some studies and methods used}


\section{Expressive Sound Synthesis}

Music is fundamentally a form of human expression [quotation needed],
and thus expressiveness a fundamental property of music.
Therefore one could argue that the ultimate goal of sound synthesis should be to support
the expressive motives of the composer and performer of the music.
However, most control parameters available in sound synthesis systems
are not related to the expressive properties of the produced sound,
but are rather based on the time-frequency properties of the signal.
Various studies have been made on the relation between
descriptive and emotional adjectives,
and the time-frequency properties of sound.
The studies have covered both the timbre of individual notes [moravec] 
and the variation timing on the notational level [friberg?].
Expression in virtual instruments is another field of study
that has had a lot of coverage. [citation needed]




\section{Conductor Follower Systems}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods}
\label{chapter:methods}

\section{Motion Tracking and Feature Extraction}

\section{Sound Synthesis}

\subsection{Tempo Following}

\subsection{Expressive Synthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\label{chapter:implementation}

\section{Architectural overview}

\section{Feature Extraction Method}

\section{Tempo Following Method}

\section{Mapping Expressions to Synthesis Parameters}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{chapter:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}



