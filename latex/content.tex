%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:intro}

The interaction between conductors and musicians
is a sophisticated form of non-verbal communication.
While the expressions used in conducting have no strict rules,
most gestures performed by experienced conductors are understood by
adequately experienced musicians.
Modelling this complex conductor-musician interaction
requires understanding the fundamental principles behind the communication.
Once the relevant features are identified,
it is possible to implement a system that
follows the movements of a conductor
by capturing and interpreting relevant data.
By combining this conductor following with
a musical score and sound synthesis system,
an interactive sonic experience can be created.

\section{Conducting Gestures}

While some forms of musical conducting have been around for hundreds of years
\cite{modernconductor},
the developments that lead to the modern form of conducting
were driven by the increasing size and complexity of symphonic scores
in the late nineteenth century \cite{gallops2005}.
The task of the modern conductor is to mold an interpretation
by guiding the musicians to play a musical score according to his vision.
This is done not only by non-verbal gestures
using body postures, hand movements, eye contact and facial expressions,
but also using verbal instructions during rehearsals.

Studying conducting from a scientific, data based approach,
is a rather recent development.
Sousa \cite{sousa1988} was among the first,
performing a study in 1988,
which investigated the use of musical conducting emblems,
and their interpretation by instrumental performers.
Sousa used videotaped conducting gestures,
which he presented to university, high school and junior high school students,
to study which gestures could be classified as conducting emblems,
non-verbal acts with precise meaning and
a common interpretation among instrumental performers.
He concluded that 38 out of the 55 studied gestures
were recognised by over 70\% of the subjects,
with the recognition rate having a strong correlation
with the experience level of the musicians.

While some conducting gestures can be easily analysed,
and do have a commonly recognized meaning,
to really understand how the communication between
conductors and musicians works,
one has to study the whole life cycle of a conducted performance -
preparations made by the conductor before rehearsing with the musicians,
rehearsal with musicians,
and finally the actual performance.
Konttinen \cite{konttinen2008} studied
conducting as a practical and sociological activity
in her dissertation \textit{Conducting Gestures}.
She came to the conclusion that the main purpose of all
conducting gestures become apparent in a social situation -
a rehearsal, performance or conducting class -
where the communicational situation includes both the gesture
and the social context in which it is used.
Therefore the meaning of conducting gestures are
heavily influenced by their social context -
the musicians affecting the way the conductor performs,
and the conductor affecting the way the musicians interpret the gestures.
Especially the rehearsal situation,
where the conductor needs to make the meaning of his gestures obvious,
is crucial for founding the basis for successful gestural communication.

\section{Gesture-Based Human-Computer Interaction}

When using gestures for human-computer interaction (HCI),
the device used for gathering data plays a big role
in the capabilities and limitations of the system.
Early gesture systems were based on input from
a camera or a specially made input device,
such as a light-pen and screen combination \cite{turk2002}.
Recent developments in technology,
such as the popularization of touchscreen smartphones,
have made gestures as a form of HCI
a part of everyday life for an increasing amount of people.
Judging by the over 600 IEEE conference publications related to gestures in 2011,
it is obvious that gestures based systems are an active field of research.

Depth sensor equipped motion sensing input devices,
such as the Mictosoft Kinect \cite{kinect_overview},
represent a recent trend of incorporating gesture based interaction
into consumer electronics.
Interaction with applications using these devices
does not require touching or wearing any equipment,
and thus applications have the potential to provide
a very natural gestural user interface.
However, the complexity of the subject lies in extracting
meaningful information out of the raw motion data provided by these devices.
For surveys on related literature,
see \cite{Moeslund2006} and \cite{Weinland2011}.

\section{Expressive Sound Synthesis}

Music is fundamentally a form of human expression,
and thus expressiveness a fundamental property of music.
Therefore one could argue that the ultimate goal of sound synthesis should be to support
the expressive motives of the composer and performer of the music.
However, most control parameters available in sound synthesis systems
are not related to the expressive properties of the produced sound,
but are rather based on the time-frequency properties of the signal.
Various studies have been made on the relation between
descriptive and emotional adjectives,
and the time-frequency properties of sound.
The studies have covered both musical cues,
such as tempo, loudness and articulation \cite{juslin2000cue},
and the timbre of individual notes \cite{moravec2005}.

The majority of motion based expressive sound synthesis implementations
have been novel approaches which do not model any existing form of musical expression.
Some have concentrated on using emotional data
extracted from the motion of the user,
to control music in a way that the
emotional features of the music would match those of the user \cite{friberg2004},
while others have provided means create mappings
between arbitrary motion data and sound features \cite{kia2004}.
\fixme{Mention virtual instruments}

Because bringing out expressive features of music is a crucial part of conducting,
a conductor follower is a rather natural candidate for
an expressive sound synthesis system.

\section{Conductor Follower Systems}

Over the years, several performance, educational and research systems
have been designed and implemented,
which follow conductor gestures and produce or modify music based on the input.
\fixme{History covered in detail in several other papers, just a quick overview here}

\fixme{Summarize what we are doing and why}

\section{Thesis Overview}

\fixme{Write some}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods}
\label{chapter:methods}

\fixme{Write some more}

\section{Make up a nice title}

If the real life interaction between a conductor and musicians
were to be modelled as closely as possible,
it would require providing a feedback channel,
equivalent to the verbal communication from conductor to musicians.
This channel could then be used to teach the
style of the specific conductor to the system.
In other words, when the system misinterprets some gesture,
the conductor could correct this interpretation by 
providing the correct interpretation via the feedback channel.
This way it would be possible for the system to
learn the style of the conductor in question over time,
using machine learning techniques.
Implementing such a system would, however,
be too large an effort for the scope of this project.
Instead, a more general and static rule based system is
considered and implemented.

\section{Motion Capture}

Motion capture in the context of the conductor follower system,
consists of tracking the hands.
\fixme{Will this still change?}
The methods used for this are fairly mature,
and not in the main focus of this thesis.
Since sufficient methods and implementations for
motion capture using depth sensor devices
are commonly available,
there is no need to cover the details of those here.

\fixme{Write more, once the implementation is more final}


\subsection{Supporting Methods}

Extracting meaningful information out of the motion data
requires using many time related analysis methods.
Extracting features such as velocity and acceleration
require inspecting the movement at a single point in time,
while the cyclic nature of conducting requires
averaging over time to extract features that apply
at the beat or bar level.

For the analysis at a single point,
the Savitzky-Golay filter \cite{savitzky1964} is used.
For averaging over time, methods such as
exponential moving averages and peak holders are used.
When selecting parameters for the averaging methods,
there is always a trade-off between the 
ability to react to fast changes
and the amount of averaging provided.
\fixme{Write more about the selection of parameters.}

\subsubsection*{The Savitzky-Golay Filter}

Luck and Toiviainen have studied the gestures of conductors
both from a beat synchronization \cite{luck2006}
and expressive \cite{luck2010} point of view.
Both studies used Savitzky-Golay filtering
for data smoothing and differentiation.
As these studies form the basis for a lot
of the analysis used in this project,
it is logical to use the same methods.

The Savitzky-Golay filter uses polynomial fitting
over a sliding window to approximate the movement
as a polynomial at each point.
This approximation is applied to each spatial axis separately,
using the points at times $t_{i-n} \ldots t_{i+n}$,
for analysing the movement at time $t_i$,
where the length of the filter is $2n + 1$.

\subsubsection*{Exponential Moving Average}

Compared to a simple moving average (SMA),
the exponential moving average (EMA)
gives two benefits in our use case:
\begin{itemize}
\item The EMA reacts to sudden changes faster than an SMA,
while sill offering good averaging.
\item The EMA is computationally much lighter, as seen below.
\end{itemize}
The EMA can be effectively calculated recursively:
\begin{equation}
S_i =
\begin{cases}
y_i & \text{if } i = 1 \\
\alpha y_i + ( 1 - \alpha ) S_{i-1} & \text{if } i > 1, \\
\end{cases}
\end{equation}
where $\alpha \in \oointerval{0, 1}$ is the smoothing coefficient,
and $y_i$ the \nth[i] observation.
High values of $\alpha$
provide less averaging with faster reaction to changes.

\subsubsection*{Peak Holder}

A peak holder simply holds the peak value
\begin{equation}
P_i = \max \left( y_i \ldots y_{i - n} \right),
\end{equation}
where $y_i$ is the \nth[i] observation,
and $n$ the length of the peak holder.
High values of $n$ will provide a smoother output,
but will not react to sudden decreases in magnitude as fast.


\subsection{Beat Detection}
\label{sec:meth:beat_detection}

The aim of the beat detection system is
to work with as many conducting styles as possible.
The approach taken here will thus not be
based on specific beat patterns or gestures.
Many previous systems have been based on
using machine learning techniques with
predefined beat patterns \cite{},
but such systems would easily get confused with
the large variance present in the style of
many orchestral conductors.
Instead, the approach is based on beat induction
from the motion features of the conductors hands.
\fixme{Write more, cite some beat induction stuff}

Luck and Toiviainen \cite{luck2006}
conducted a beat synchronization study,
which was novel in the sense that it studied
the subject in a real orchestral rehearsal situation.
Previous studies have been made primarily in laboratory settings,
and thus do not present a good foundation
for emulating the behavior of an orchestra.
The study analyzed the motion of the baton
in four excerpts of conducting,
collected from one rehearsal situation.
Since the study represented the conducting style of only one conductor,
and the response of only one orchestra,
it does not present universally valid results.

\fixme{
This part needs more research.
Low position, y-acceleration and acceleration along trajectory
are the stuff we want to look into.
New Perspectives on Music and Gesture
has an article by Luck, which at least has references.
Remember, correlation (esp. decceleration along trajectory)
does not mean it can be used for detection!
$v_y$ and $a_t$ together would probably give good
input for a learning system.
Would we still like to go there?
}

Luck and Toiviainen's study showed that when the conductor's gestures were well articulated,
the lowest vertical position preceded the beat by circa 230 milliseconds,
while the highest vertical velocity preceded the beat by circa 90 milliseconds,
both having a high correlation at that lag.

\subsection{Start Gesture Detection}

The gesture for starting the piece
is detected using the beat detection method, together with
the lowest vertical position preceding the beat and
the highest vertical position succeeding the beat.
If the following criteria are filled,
a start gesture is detected:
\begin{itemize}
\item The latest minimum of the vertical position was before the beat.
\item The latest maximum of the vertical position was after the beat.
\item The distance between the minimum and maximum is large enough.
\item The time between the minimum and maximum is within a suitable range.
This range is based on the start tempo estimation described in section
\ref{sec:meth:start_tempo_estimation}.
\end{itemize}

\subsection{Expressive Feature Extraction}

Luck and Toiviainen \cite{luck2010}
studied the correlation between the kinematic features of conductors' hands
and the perceived magnitude of different expressions in their movement.
The studied expressions were
\textit{Expression}, \textit{Valence}, \textit{Activity} and \textit{Power}.
A point-light representation of two conductors' movements
was presented to subjects, who rated the expressiveness continuously.
The study showed correlations between kinematic features
such as position, velocity, acceleration and jerk,
and the afore mentioned expressions.
However, the final conclusions of the study stated,
that the most essentials findings were that observers
are indeed sensitive to more fine-grained kinematic features,
and that increased amplitude, greater variance and higher speed of movement
convey higher levels of expressivity.

\fixme{This paragraph should be revised later}
Based on the results by Luck and Toiviainen \cite{luck2010},
and Sousa \cite{sousa1988},
a set of kinematic features were selected for extracting
expressive features out of the motion data.
\fixme{Write more}

\section{Score Following}
\label{sec:meth:score_following}

\fixme{Write something}

\subsection{Score Data Format}

Regardless of its limitations and age,
MIDI (Musical Instrument Digital Interface)
is still one of the most used standards
for the digital representation of of musical events.
MIDI is also the built-in way to communicate musical events
in the Virtual Studio Technology (VST) plugin format,
which is used by VSL.
Thus using MIDI as the score format was a natural choice.

A MIDI score does contain
note, time signature and tempo information.
It may also contain \textit{program change} events,
as specified in the General Midi specification \cite{},
to indicate the instrument to be used for each track.
However, practical experience with MIDI files showed,
that using program changes to deduce the instrument to use,
was not sufficient.
Often scores would use the
\textit{String ensemble} patch for all strings,
even though the string instruments were separated to individual tracks.
In cases like this it is necessary to be able to manually
define the instrument to be used with each track.

In addition to track instrument assignment,
the system should also be aware of events in the score.
\fixme{Write about score events}

\subsection{Real Time and Score Positions}
\label{sec:meth:read_and_score_time}

When modifying the playback speed of a musical score
which has tempo variations already build into it,
it is important to precisely define the concepts of time used.
Wall-clock time is the monotonically progressing concept of time
most familiar to humans.
It is the time counted by a regular clock,
and will be referred to as real time,
its dimension denoted by $T$ and quantities with $t$.

Score positions, on the other hand,
are slightly more complex.
Musical events in a fully defined score have two time bases:
\begin{enumerate}
\item Musical time has the base unit of beats, but also includes a time varying derived unit of bars. Any position in the score can thus be expressed as either a beat offset, or a combination of bars and beats. Its dimension is denoted with $B$ and quantities with $b$.
\item Score time has the base unit of seconds. This is an offset from the beginning of the score if the original tempo is followed. Its dimension is denoted with $\Phi$ and quantities with $\varphi$.
\end{enumerate}
A score position is thus defined as
$ \tupledef{P}{b, \varphi}, $
and variables will be denoted with a $p$.

Mapping between real time and score positions will be denoted with
the mapping operator
$ \mathcal{M} : T \rightarrow P, $
and it's inverse
$ \mathcal{M}^{-1} : P \rightarrow T. $

The mapping operator (as well as its inverse)
requires knowledge of
the realized playback speed,
and can thus be applied reliably to
past events.
It is also possible to use the mapping operator for
estimating future events.
These estimates, however,
may produce results that differ from
from future mapping operations,
and may only be used in special cases.

The relation between real time and score time
is embedded in the mapping operator.
To formalize the relation, we can
represent the relation with a warp factor
\begin{equation}
w = \frac{\Delta \phi}{\Delta t},
\end{equation}
which is piecewise constant over time.
 

\subsection{Tempo Following}

The aim of the tempo following system
is to behave like a real orchestra would,
when reacting to a conductor.
\fixme{quote some studies on this matter, if you find some}
This implies that
\begin{enumerate}
\item The orchestra should not be confused by changes in the conducting pattern. \label{follow_point:pattern}
\item In the absence of beats, the orchestra will continue with it's previous tempo.
\item Extra beats in "unexpected" positions will not be acted upon. \label{follow_point:unexpected}
\item Sudden changes in tempo will not be immediately followed.
\end{enumerate}
Points \ref{follow_point:pattern} and \ref{follow_point:unexpected} require
that the tempo follower must have knowledge of all the possible
conducting patterns for each time signature,
and that it must be able to relate each beat to some beat in the pattern.
\fixme{Write more}

The formal objective of tempo following
is to minimize the offset between
beats conducted and
beats played by the virtual orchestra.
Once the beats are detected as described
in section \ref{sec:meth:beat_detection},
the beats have to be classified.
A beat classification is defined as the tuple
\[
\tupledef{C}{\Delta b, q, \tau},
\]
where $\Delta b$ is the offset,
$q$ the classification quality,
and $\tau$ the type
\[
\setdef{\tau}{\mathrm{NotClassified}, \mathrm{CurrentBar}, \mathrm{NextBar}}
\]
Each beat pattern has a related
classification operation $ \mathcal{C} : B \rightarrow C, $
which uses the relative position of the detected beat
within the current bar,
to produce a classification.
Out of the set of classifications produced by the beat patterns,
the classification with the best quality is
selected as the final classification.

The action taken based on the winning classification
depends on the type of the classification.
In the case of NotClassified, no action is taken for that beat.
When the type is NextBar,
the current bar is incremented,
and the classification is repeated with an
updated relative beat position.
The type CurrentBar,
means that the offset $\Delta b$ is used for correcting
the current warp factor $w$.
The factor is corrected so that after a catchup time $t_c$,
the position will differ by $\Delta b$,
compared to the position
that would be reached by maintaining the current tempo.
The correction is applied as a tempo function $\upsilon (t)$,
whose magnitude is derived from the integral
\begin{equation}
\int_t^{t + t_c} \upsilon \, \mathrm{d}t = \Delta b.
\end{equation}

Given the previous beat offset $\Delta b'$ (at time $t'$),
and current $\upsilon (t)$,
the current offset can be estimated as
\begin{equation}
\Delta b(t) = \Delta b' - \int_{t'}^t \upsilon \, \mathrm{d}t.
\end{equation}
To get the best beat classifications,
$ \mathcal{C} $ is used with the relative beat position
\begin{equation}
b' = \mathcal{M} [ t ] - p_B - \Delta b(t),
\end{equation}
where $t$ is the time of the beat,
and $p_B$ the beginning of the current bar.

The classification operation $\mathcal{C}$,
uses the beat positions within the bar $\mathbf{b}$,
to create an estimate for the beat position $b$:
\begin{enumerate}
\item The classification for the current bar $c_c$,
is calculated by selecting the most probable match for $b$
from the available positions $\mathbf{b}$.
\begin{enumerate}
\item The offset for each beat is calculated:
\begin{equation}
\Delta b_i = b - \mathbf{b}_i.
\end{equation}

\item The quality of each beat is calculated as
\begin{equation}
q_i = -\vert \Delta b_i \vert.
\end{equation}

\item If the beat $\mathbf{b}_i$ has already been marked as used,
a beat penalty $q_{p_b}$ is added to $q_i$,
and the type $\tau_i$ is set as $\mathrm{NotClassified}$,
in order to not classify two beats as the same.
Otherwise $\tau_i$ is set as $\mathrm{CurrentBar}$.

\item The classification is then selected as
\begin{equation}
c_c = \tuple{\Delta b_j, q_j, \tau_i},
\end{equation}
where $j$ is the index with the best quality,
and the beat $\mathbf{b}_j$ is marked as used.

\end{enumerate}

\item For each previously unused beat $\mathbf{b}_i \mid i < j$,
where $j$ is the index of $c_c$,
a bar penalty $q_{p_B}$ is added to the bar quality,
and the beat is marked as used.

\item The classification for moving to the next bar $c_n$,
is calculated from the offset to the next bar.
\begin{enumerate}
\item The offset is calculated based on the length of the bar $b_B$,
\begin{equation}
\Delta b_n = b - b_B.
\end{equation}

\item The classification is built based on this offset
\begin{equation}
c_n = \tuple{\Delta b_n, -\vert \Delta b_n \vert, \mathrm{NextBar}}.
\end{equation}

\end{enumerate}

\item The final classification $c_f$,
is selected from $ \set{c_c, c_n} $,
maximizing the quality.

\item The quality of $c_f$ is added to the quality of this bar.

\end{enumerate}
Once the classification has been done for each beat pattern,
the beat pattern with the best quality for the current bar
is selected and acted upon.
The bar qualities are reset when the winning classification
is of type $\mathrm{NextBar}$,
and are updated at every detected beat as described above.

\subsection{Start Tempo Estimation}
\label{sec:meth:start_tempo_estimation}

The start tempo is estimated based on
the duration of the start gesture.
Th duration from bottom to top position is
assumed to be the length of one eight beat,
and the starting tempo thus
\begin{equation}
v_s = \tfrac{1}{2} (t_t - t_b),
\end{equation}
where $t_t$ is the time at the top of the gesture,
and $t_b$ the time at the bottom of the gesture.

\fixme{Will the tempo estimation in the first bar(s) be changed?}

\section{Sound Synthesis}

Since the objective of the system is to emulate an orchestra,
also the sound synthesis system should be able to
provide realistic synthesis of an entire orchestra.
It should be able to reproduce a musical composition
at varying tempi and articulations.

There are several commercial orchestral synthesizers available,
the majority of which use some form of sampling or concatenative synthesis.
Based on the feature set provided and
subjective evaluation of sound quality,
Vienna Symphonic Library (VSL) \cite{vsl} was chosen for the task.
VSL uses a form of concatenative synthesis,
using a vast library of samples played at
various velocities and articulations \cite{schwartz2006}.

Each instrument in VSL contains a set of \textit{patches},
which represent a certain articulation and/or playing style.
Each patch is capable of synthesizing
the whole scale of the given instrument at several velocities.
Some patches (such as \textit{staccato} articulations)
have a limited note length,
while others allow stretching out the note infinitely.
Switching between patches can be accomplished with \textit{keyswitch} events
(notes outside of the scale of the instrument).

In order to control the switching of patches,
the synthesis system must have some knowledge of
the available instruments and their patches.
The parameters used for describing the patches are:
\begin{description}
\item[Length] The maximum note length allowed by the patch.
\item[Attack] The sharpness of the attack portion of the notes.
\item[Weight] The \textit{musical weight} of the note, i.e. an accented note would have a large weight.
\end{description}
By using this information,
together with the current motion capture and score state,
the system is capable of selecting
the most appropriate patch for each note.

\fixme{Write more on patch selection (e.g. distance function)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\label{chapter:implementation}

Since the synthesis environment is a VST plugin,
implementing the conductor follower as a VST plugin was a logical choice.
The input for the plugin is read from a MIDI file
and the output is MIDI events via the VST interface.
These implementation details are, however, hidden behind
carefully designed interfaces,
separating the core functionality from the input/output functionality.

One of the most important considerations of the implementation
is its real-time (RT) constraints.
A RT system is defined as a system where calculations
need to be completed before a given deadline,
and can be classified into three categories:
\begin{description}
\item[Hard]
Missing a deadline is a total system failure.
\item[Firm]
The usefulness of a result is zero after its deadline.
\item[Soft]
The usefulness of a result degrades after its deadline.
\end{description}
An audio plugin has firm real-time constraints,
since the result for each block of audio needs to be delivered in time.
Missing a deadline means that block will not be played back,
causing a glitch in the sound output.

It is important to understand the difference between
deadline based real time constraints
and throughoutput based constraints.
Throughoutput based real time constraints have
more to do with with the algorithmic complexity
of the methods used,
while deadline based constraints
require special programming techniques on the implementation level.
All further references to RT constraints in this thesis
refer to the deadline based definition.

In addition to having RT constraints,
the 30Hz frame rate of the motion capture system
makes it a multirate, multithreaded application.
Working in such an environment requires using
lock-free programming techniques,
such as atomic variables, read-copy-update (RCU) constructs,
and lock-free ringbuffers.
The multirate nature also requires having robust timestamping and synchronization mechanisms.

\fixme{chapter overview}

\section{Supporting Libraries}

The implementation makes heavy use of several of the Boost C++ libraries \cite{boost}.
The most noteworthy ones are described in the following list.
Libraries included in the C++11 standard \cite{cpp11}
are denoted with $^*$,
while libraries not yet in the official boost distribution
are marked with $^\dagger$.
\begin{description}[leftmargin=14ex]
\item[Chrono$^*$] Time library for timestamping and jitter correction.
\item[Geometry] \fixme{Is this really used much anywhere?}
\item[Lockfree$^\dagger$] Various lock-free constructs.
\item[Spirit] A parsing library, used for configuration file parsing.
\item[Thread$^*$] Threading utilities.
\item[uBLAS] Linear algebra, used for polynomial fitting.
\item[Units] Compile-time dimensional analysis.
\end{description}

The OpenNI Framework \cite{openni} is an open source cross-platform
framework for Natural Interaction (NI) devices.
PrimeSense Ltd \cite{primesense} provides a proprietary
middleware package called NITE,
which works with the OpenNI API,
providing higher level functionality such as skeletal and hand tracking.
NITE is used via the OpenNI APIs for hand tracking.

Juce \cite{juce} is an open source,
multimedia oriented cross-platform C++ library.
It was used for its
VST plugin wrapper, MIDI file I/O,
and user interface (UI) functionality.

\section{Architectural overview}

The implementation is divided into
six well defined and loosely coupled modules:
\begin{itemize}
\item Common utilities
\item Data file parsers
\item Motion capture
\item Expression to synthesis parameter mapper
\item Score Follower (the core functionality)
\item The VST plugin
\end{itemize}

The common utilities module contains components
that are not specific to conductor following.
These include utilities for
lock-free programming,
mathematical operations and algorithms,
time and geometry handling,
logging, and debugging.

The data file parser module
contains all the parser definitions
and supporting data structures
for all the input files used by the system.
Its main purpose is to hide the implementation
of parsing behind a concise API.

The motion capture module provides a simple
event-based API for motion related data.
It includes the logic for extracting relevant events from motion data.
It also contains all the code that uses the OpenNI APIs,
so that the rest of the system has no dependencies on OpenNI.

The expression to synthesis parameter mapper
maps expression parameters to synthesis parameters.
\fixme{Write more}

The score follower module provides most of the core functionality,
not including motion capture.
This includes
beat classification, tempo and score following, jitter correction,
and instrument patch switching among other things.
It also provides abstract interfaces for 
hiding the implementation details of the score format.

The VST plugin modules main purpose is to
separate as much plugin implementation detail from the rest of the system.
It implements the VST interface and provides the plugin UI,
both using Juce.
It also implements the score related interfaces,
defined in the score follower module,
using Juce MIDI functionality.

In addition to these six modules,
the project contains four unit test modules.
These modules are not significant regarding the functionality,
but merely had a supporting role during system development.

\subsection{Thread model}

The VST architecture implies using at least two threads:
one for audio and/or MIDI processing,
and another for the UI.
In addition to these two threads,
motion capture needs it's own thread,
and one additional thread is needed for some
lockfree programming techniques.
To summarize, the threads are:
\begin{enumerate}

\item VST MIDI
\begin{itemize}
\item Provided by plugin host.
\item Produces the MIDI events.
\item Has firm RT requirements.
\item Low latency required.
\end{itemize}

\item VST UI
\begin{itemize}
\item Provided by plugin host.
\item Renders the plugin UI.
\end{itemize}

\item Motion Capture
\begin{itemize}
\item Runs the motion capture device.
\item Has firm RT requirements.
\item Higher latency than the MIDI thread (30 Hz).
\end{itemize}

\item Butler
\begin{itemize}
\item Runs asynchronous tasks for the RT threads.
\end{itemize}

\end{enumerate}

\subsection{Essential common utilities}

\subsubsection*{Event Buffer}

\section{Time Handling}

Handling time in a multirate, multi-threaded system,
is not a trivial task.
The motion capture thread will be in
a blocking state most of the time,
waking up at a 30Hz frequency to process data
provided by the motion capture system.
The plugin, however, needs to provide MIDI data
based on the host's audio settings,
a typical configuration running the
plugin at around 100 Hz (an audio block length of 10ms).
Additionally, the thread in which the plugin runs
is controlled by the host,
and the code has to be absolutely lock-free.

Considering the implications of needing to run lock-free code on
a modern multi-core system running a general purpose
operating system (OS),
no assumptions about causality or parallelism between
motion capture and plugin code execution can be made --
scheduling latencies and parallel execution forces the use
of a reliable timestamping mechanism for synchronization between
the audio and motion capture threads.
The Boost Chrono library provides a steady clock,
which measures time since the latest system boot-up.
Acquiring the current timestamp was verified to be lock-free
at least on Windows and OS X as of Boost version 1.49.0.
These timestamps are used for measuring the interval between
events happening in the motion capture and plugin threads.
Timestamping events in the motion capture thread is straightforward,
each event simply being timestamped with
the current timestamp returned by the system.
The plugin thread will, however,
require jitter correction to prevent timing problems.

\subsection{Jitter Correction}

The plugin thread needs to know not only
the timestamp corresponding to the beginning of the current audio block,
but also an estimate for the end of the block.
Let the audio block be defined by its beginning and end:
\begin{equation}
\boldsymbol{\tau} = [\tau_b, \tau_e[.
\end{equation}
Using the information provided to the plugin,
the theoretical block length
\begin{equation}
\label{eq:theoretical_block_len}
\Delta \tau' = \frac{n_b}{f_s},
\end{equation}
where $n_b$ is the block size in samples and $f_s$ the samplerate.
This length is then used for estimating the end of each block
\begin{equation}
\tau_e = t_c + \Delta \tau',
\end{equation}
where $t_c$ is the current timestamp at the beginning of the block.
Due to scheduling latencies,
the value of $t_c$ might differ from the
previous block end estimate $\tau_{e_{\mathrm{prev}}}$.
In order to prevent gaps and overlaps between the
time estimates of consecutive audio blocks,
the beginning of each block is taken from the end of the previous block
\begin{equation}
\tau_b = \tau_{e_{\mathrm{prev}}}.
\end{equation}
The VST specification requires
each MIDI event to have its position defined
as a sample offset from the beginning of the current block.
As the actual block length
\begin{equation}
\Delta \tau = \tau_e - \tau_b
\end{equation}
will differ from the theoretical block length
$\Delta \tau'$ most of the time,
this difference will need to be taken into consideration
when calculating the event sample offset
\begin{equation}
\Delta n_e = \frac{\Delta \tau'}{\Delta \tau} (t_e - \tau_b) f_s,
\end{equation}
where $t_e$ is the event time. By using equation \ref{eq:theoretical_block_len},
this can be simplified to
\begin{equation}
\Delta n_e = n_b \frac{t_e - \tau_b}{\Delta \tau}.
\end{equation}

\subsection{Dimensional Analysis}

Because of the various time bases and units used
(see section \ref{sec:meth:read_and_score_time}),
using compile time dimensional analysis using Boost.Units \cite{needed?}
eased the development process.
Boost.Units uses zero runtime overhead C++ template metaprogramming \cite{abrahams?}
techniques to check during compile time,
that all calculations have the proper dimensions.
This does not guarantee dimensionally proper calculations all the time
(e.g. dimensionless units may cause errors),
but does catch many errors made during development.
For handling time,
a custom unit system was declared.
This system contains base dimensions for
beats, bars, samples and physical time,
and derived dimensions for
bar durations, tempi, sample rates, and speed changes.
The unit system is described in detail in Table \ref{tab:score_units}.
The difference between real time and score time (both in seconds),
comes inherently from the libraries used:
Boost.Units for score time and Boost.Chrono for real time.
This implies that an explicit conversion function is always
needed to convert between these two dimensions.

\begin{table}
\begin{center}
\begin{tabular}{ | l  l  p{5.5cm} |}
\hline
Dimension & Base Unit & Notes \\ \hline
beats & beat & Base for musical time \\
bars & bar & See bar duration. \\
samples & sample & See sample rate\\
time & second & Base for physical time \\
bar duration & beats per bar & Derived dimension. \\
tempo & beats per second & Derived dimension. \\
sample rate & samples per second & Derived dimension. \\
speed change & fractions per second & Fraction here is a dimensionless unit. A derived dimension. \\
\hline
\end{tabular}
\caption{Overview of custom unit system for time.}
\label{tab:score_units}
\end{center}
\end{table}

\section{Motion Capture}

\subsection{The Savitzky-Golay Filter}

The Savitzky-Golay filter is used for three purposes in the system:
smoothing, differentiation and interpolation.
The polynomial fitting part for the filter is implemented using
matrix operations and the uBLAS library.
Estimating the polynomial coefficients is done by the equation
\begin{equation}
\label{eq:sg_coefs}
\widehat{\mathbf{a}} = \left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T \mathbf{y},
\end{equation}
where $\widehat{\mathbf{a}}$ are the estimated coefficients,
$\mathbf{y}$ the observations,
and $\mathbf{X}$ the design matrix
\begin{equation}
\label{eq:sg_design_matrix}
\mathbf{X} = \left[
\begin{array}{ccccc}
1 & x_1 & x_1^2 & \ldots & x_1^m \\
1 & x_2 & x_2^2 & \ldots & x_2^m \\
1 & x_3 & x_3^2 & \ldots & x_3^m \\
\vdots & \vdots & \vdots & & \vdots \\
1 & x_n & x_n^2 & \ldots & x_n^m \\
\end{array}
\right] ,
\end{equation}
where $x_1 \ldots x_n$ are the observation times,
and $m$ the order of the resulting polynomial.
It is required that $n > m$.
Smoothing and differentiation is implemented by
selecting an odd value for $n$
and evaluation the polynomial at the center value
$x_{\lfloor \frac{n}{2} \rfloor}$.
An optimization that is easy to implement for equally sampled data,
is to pre-calculate $\left( \mathbf{X}^T \mathbf{X} \right)^{-1} \mathbf{X}^T$,
with $x_{\lfloor \frac{n}{2} \rfloor} = 0$.
The value and derivatives at the center point
can thus be calculated from the polynomial
\begin{equation}
\widehat{y}(x) =
\widehat{\mathbf{a}}_m \, x^m +
\widehat{\mathbf{a}}_{m-1} \, x^{m-1} +
\ldots +
\widehat{\mathbf{a}}_{2} \, x +
\widehat{\mathbf{a}}_{1}.
\end{equation}
The simplified equations being
\begin{equation}
\widehat{y}(0) = \widehat{\mathbf{a}}_{1}
\end{equation}
for the value, and
\begin{equation}
\widehat{y}^{(n)}(0) = n! \: \widehat{\mathbf{a}}_{n + 1}
\end{equation}
for the \nth derivative.

To make $\mathbf{X}^T \mathbf{X}$ invertible,
it is also necessary to reverse the values $x_1 \ldots x_n$
used in equation \ref{eq:sg_design_matrix}.
This change requires a correction to equation \ref{eq:sg_coefs},
which is reversing the observation values $\mathbf{y}$.

\subsection{Beat Detection}

\subsection{Expressive Features}

\section{Score Following}

The implementation of the score following methods
described in section \ref{sec:meth:score_following}
is rather straight forward.



\subsection{Beat Classification}

\subsection{Time Warping}



% Use \textofpdfstring when the title is final,
% to add a nice line break
\section{Mapping Expressions to Synthesis Parameters}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{chapter:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}



