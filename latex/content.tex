%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
\label{chapter:intro}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chapter:background} 

The interaction between conductors and musicians
is a sophisticated form of non-verbal communication.
While the expressions used in conducting have no strict rules,
most gestures performed by an experienced conductor are understood by
adequately experienced musicians.
In order to model this complex conductor-musician interaction,
it is vital to understand the fundamental principles behind the communication.
It is then possible to capture relevant data
and combine it with a musical score to
produce an interactive sonic experience.

\section{Conducting Gestures}

\fixme{Summarize what a modern conductor does}

While some forms of musical conducting have been around for hundreds of years
[Green 1961, look up from gallops 2005],
the developments that lead to the modern form of conducting
were driven by the increasing size and complexity of symphonic scores
in the late nineteenth century. \cite{gallops2005}
Studying conducting from a scientific, data based approach,
is an even more recent development.
Sousa \cite{sousa1988} was among the first,
performing a study in 1988,
which investigated the use of musical conducting emblems,
and their interpretation by instrumental performers.
Sousa used videotaped conducting gestures,
which he presented to university, high school and junior high school students,
to study which gestures could be classified as conducting emblems,
non-verbal acts with precise meaning and
a common interpretation among instrumental performers.
He concluded that 38 out of the 55 studied gestures
were recognised by over 70\% of the subjects,
with the recognition rate having a strong correlation
with the experience level of the musicians.

While some conducting gestures can be easily analysed,
and do have a commonly recognised meaning,
to really understand how the communication between
conductors and musicians works,
one has to study the whole life cycle of a conducted performance -
preparations made by the conductor before rehearsing with the musicians,
rehearsal with musicians,
and finally the actual performance.
Konttinen \cite{konttinen2008} studied
conducting as a practical and sociological activity
in her dissertation \textit{Conducting Gestures}.
She came to the conclusion that the main purpose of all
conducting gestures become apparent in a social situation -
a rehearsal, performance or conducting class -
where the communicational situation includes both the gesture
and the social context in which it is used.
Therefore the meaning of conducting gestures are
heavily influenced by their social context -
the musicians affecting the way the conductor performs,
and the conductor affecting the way the musicians interpret the gestures.
Especially the rehearsal situation,
where the conductor needs to make the meaning of his gestures obvious,
is crucial for founding the basis for successful gestural communication.

\section{Gesture-Based Human-Computer Interaction}

When using gestures for human-computer interaction,
the device used for gathering data plays a big role
in the capabilities and limitations of the system.
Early gesture systems were based on input from
a camera or a specially made input device [citation needed].
Recent developments in technology,
such as popularization of touchscreen smartphones,
have made gestures as a form of human-computer interaction
a part of everyday life for an increasing amount of people.

Depth sensor equipped motion sensing input devices,
such as the Mictosoft Kinect \cite{kinect_overview},
represent a recent trend of incorporating gesture based interaction
into consumer electronics.
Interaction with applications using these devices
does not require touching or wearing any equipment,
and thus the applications have the potential to provide
a very natural gestural user interface.
However, the complexity of the matter lies in extracting
meaningful information out of the raw motion data provided by these devices.
\fixme{quote some studies and methods used}


\section{Expressive Sound Synthesis}

Music is fundamentally a form of human expression [quotation needed],
and thus expressiveness a fundamental property of music.
Therefore one could argue that the ultimate goal of sound synthesis should be to support
the expressive motives of the composer and performer of the music.
However, most control parameters available in sound synthesis systems
are not related to the expressive properties of the produced sound,
but are rather based on the time-frequency properties of the signal.
Various studies have been made on the relation between
descriptive and emotional adjectives,
and the time-frequency properties of sound.
The studies have covered both musical cues,
such as tempo, loudness and articulation \cite{juslin2000cue},
and the timbre of individual notes \cite{moravec2005}.

The majority of motion based expressive sound synthesis implementations
have been novel approaches which do not model any existing form of musical expression.
Some have concentrated on using emotional data
extracted from the motion of the user,
to control music in a way that the
emotional features of the music would match those of the user \cite{friberg2004},
while others have provided means create mappings
between arbitrary motion data and sound features \cite{kia2004}.
\fixme{Mention virtual instruments}

Because bringing out expressive features of music is a crucial part of conducting,
a conductor follower is a rather natural candidate for
an expressive sound synthesis system.


\section{Conductor Follower Systems}

Over the years, several performance, educational and research systems
have been designed and implemented,
which follow conductor gestures and produce or modify music based on the input.
\fixme{History covered in detail in several other papers, just a quick overview here}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods}
\label{chapter:methods}

\section{Motion Tracking and Feature Extraction}

\section{Sound Synthesis}

\subsection{Tempo Following}

\subsection{Expressive Synthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Implementation}
\label{chapter:implementation}

\section{Architectural overview}

\section{Feature Extraction Method}

\section{Tempo Following Method}

\section{Mapping Expressions to Synthesis Parameters}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion}
\label{chapter:discussion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions}
\label{chapter:conclusions}



